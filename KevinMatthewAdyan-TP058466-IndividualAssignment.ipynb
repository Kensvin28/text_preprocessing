{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Form Tokenisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sentence tokenizer from nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text from Data_1.txt\n",
    "with open('Data_1.txt') as f:\n",
    "    data_1 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis is \"contextual mining of text which identifies and extracts subjective information\" in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations.\n",
      "\n",
      "However, analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics.\n",
      "\n",
      "This is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered.\n",
      "\n",
      "So what should a brand do to capture that low hanging fruit?\n"
     ]
    }
   ],
   "source": [
    "# Segment text into sentences using nltk sentence tokenizer\n",
    "sentences = sent_tokenize(data_1)\n",
    "\n",
    "# Display the list, with 2 new line characters to separate each sentence in the list\n",
    "print(*sentences, sep = \"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Word Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text from Data_1.txt\n",
    "with open('Data_1.txt') as f:\n",
    "    data_1 = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', '\"contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information\"', 'in', 'source', 'material,', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand,', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations.', 'However,', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics.', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit?']\n"
     ]
    }
   ],
   "source": [
    "# Split text by whitespace using split function\n",
    "tokens = data_1.split()\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', 'contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information', 'in', 'source', 'material', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations', 'However', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit']\n"
     ]
    }
   ],
   "source": [
    "# Import regular expression module\n",
    "import re\n",
    "\n",
    "# Find all words from the text\n",
    "tokens = re.findall(\"[\\w]+\", data_1)\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', '``', 'contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information', \"''\", 'in', 'source', 'material', ',', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations', '.', 'However', ',', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics', '.', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered', '.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit', '?']\n"
     ]
    }
   ],
   "source": [
    "# Import word tokenizer from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Segment text into words using nltk word tokenizer\n",
    "words = word_tokenize(data_1)\n",
    "print(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Form Word Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming:\n",
      " ['Sentiment', 'analysis', 'is', '``', 'contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information', \"''\", 'in', 'source', 'material', ',', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations', '.', 'However', ',', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics', '.', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered', '.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit', '?']\n",
      "\n",
      "After Regex stemming:\n",
      " ['Sentiment', 'analysi', 'is', '``', 'contextu', 'min', 'of', 'text', 'which', 'identifie', 'and', 'extract', 'subject', 'inform', \"''\", 'in', 'sourc', 'materi', ',', 'and', 'help', 'a', 'busines', 'to', 'understand', 'the', 'soci', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'servic', 'whil', 'monitor', 'onlin', 'convers', '.', 'However', ',', 'analysi', 'of', 'soci', 'media', 'stream', 'is', 'usual', 'restrict', 'to', 'just', 'basic', 'sentiment', 'analysi', 'and', 'count', 'bas', 'metric', '.', 'Thi', 'is', 'akin', 'to', 'just', 'scratch', 'the', 'surfac', 'and', 'miss', 'out', 'on', 'thos', 'high', 'valu', 'insight', 'that', 'are', 'wait', 'to', 'be', 'discover', '.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'captur', 'that', 'low', 'hang', 'fruit', '?']\n",
      "\n",
      "After Porter stemming:\n",
      " ['sentiment', 'analysi', 'is', '``', 'contextu', 'mine', 'of', 'text', 'which', 'identifi', 'and', 'extract', 'subject', 'inform', \"''\", 'in', 'sourc', 'materi', ',', 'and', 'help', 'a', 'busi', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'servic', 'while', 'monitor', 'onlin', 'convers', '.', 'howev', ',', 'analysi', 'of', 'social', 'media', 'stream', 'is', 'usual', 'restrict', 'to', 'just', 'basic', 'sentiment', 'analysi', 'and', 'count', 'base', 'metric', '.', 'thi', 'is', 'akin', 'to', 'just', 'scratch', 'the', 'surfac', 'and', 'miss', 'out', 'on', 'those', 'high', 'valu', 'insight', 'that', 'are', 'wait', 'to', 'be', 'discov', '.', 'so', 'what', 'should', 'a', 'brand', 'do', 'to', 'captur', 'that', 'low', 'hang', 'fruit', '?']\n",
      "\n",
      "After Snowball stemming:\n",
      " ['sentiment', 'analysi', 'is', '``', 'contextu', 'mine', 'of', 'text', 'which', 'identifi', 'and', 'extract', 'subject', 'inform', \"''\", 'in', 'sourc', 'materi', ',', 'and', 'help', 'a', 'busi', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'servic', 'while', 'monitor', 'onlin', 'convers', '.', 'howev', ',', 'analysi', 'of', 'social', 'media', 'stream', 'is', 'usual', 'restrict', 'to', 'just', 'basic', 'sentiment', 'analysi', 'and', 'count', 'base', 'metric', '.', 'this', 'is', 'akin', 'to', 'just', 'scratch', 'the', 'surfac', 'and', 'miss', 'out', 'on', 'those', 'high', 'valu', 'insight', 'that', 'are', 'wait', 'to', 'be', 'discov', '.', 'so', 'what', 'should', 'a', 'brand', 'do', 'to', 'captur', 'that', 'low', 'hang', 'fruit', '?']\n",
      "\n",
      "After Lancaster stemming:\n",
      " ['senty', 'analys', 'is', '``', 'context', 'min', 'of', 'text', 'which', 'ident', 'and', 'extract', 'subject', 'inform', \"''\", 'in', 'sourc', 'mat', ',', 'and', 'help', 'a', 'busy', 'to', 'understand', 'the', 'soc', 'senty', 'of', 'their', 'brand', ',', 'produc', 'or', 'serv', 'whil', 'monit', 'onlin', 'convers', '.', 'howev', ',', 'analys', 'of', 'soc', 'med', 'streams', 'is', 'us', 'restrict', 'to', 'just', 'bas', 'senty', 'analys', 'and', 'count', 'bas', 'met', '.', 'thi', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surfac', 'and', 'miss', 'out', 'on', 'thos', 'high', 'valu', 'insight', 'that', 'ar', 'wait', 'to', 'be', 'discov', '.', 'so', 'what', 'should', 'a', 'brand', 'do', 'to', 'capt', 'that', 'low', 'hang', 'fruit', '?']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text from Data_1.txt\n",
    "with open('Data_1.txt') as f:\n",
    "    data_1 = f.read()\n",
    "\n",
    "# Import stemmers from nltk\n",
    "from nltk.stem import PorterStemmer, RegexpStemmer, LancasterStemmer, SnowballStemmer\n",
    "# Import word tokenizer from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Segment text into words using nltk word tokenizer\n",
    "words = word_tokenize(data_1)\n",
    "print(\"Before stemming:\\n\", words)\n",
    "print()\n",
    "\n",
    "# Initialise Regex stemmer\n",
    "regexp = RegexpStemmer('ing$|s$|e$|able$|ed$|ly$|al$|ive$|ations?$', min=4)\n",
    "print(\"After Regex stemming:\\n\", [regexp.stem(w) for w in words])\n",
    "print()\n",
    "\n",
    "# Initialise Porter stemmer\n",
    "porter = PorterStemmer()\n",
    "print(\"After Porter stemming:\\n\", [porter.stem(w) for w in words])\n",
    "print()\n",
    "\n",
    "# Initialise Snowball stemmer\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "print(\"After Snowball stemming:\\n\", [snowball.stem(w) for w in words])\n",
    "print()\n",
    "\n",
    "# Initialise Lancaster stemmer\n",
    "lancaster = LancasterStemmer()\n",
    "print(\"After Lancaster stemming:\\n\", [lancaster.stem(w) for w in words])\n",
    "print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Filter stop words and punctuation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Remove stopwords and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered text corpus:\n",
      "['Sentiment', 'analysis', 'contextual', 'mining', 'text', 'identifies', 'extracts', 'subjective', 'information', 'source', 'material', 'helping', 'business', 'understand', 'social', 'sentiment', 'brand', 'product', 'service', 'monitoring', 'online', 'conversations', 'However', 'analysis', 'social', 'media', 'streams', 'usually', 'restricted', 'basic', 'sentiment', 'analysis', 'count', 'based', 'metrics', 'akin', 'scratching', 'surface', 'missing', 'high', 'value', 'insights', 'waiting', 'discovered', 'brand', 'capture', 'low', 'hanging', 'fruit']\n"
     ]
    }
   ],
   "source": [
    "# Text from Data_1.txt\n",
    "with open('Data_1.txt') as f:\n",
    "    data_1 = f.read()\n",
    "\n",
    "# Import stopwrods from nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Import word tokenizer from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Remove punctuation\n",
    "import string\n",
    "data_1 = data_1.translate(str.maketrans(\"\", \"\",string.punctuation))\n",
    "\n",
    "# Segment text into words using nltk word tokenizer\n",
    "tokens = word_tokenize(data_1)\n",
    "\n",
    "# English stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# Remove stopwords\n",
    "content = [word for word in tokens if word.lower() not in stopwords_list]\n",
    "print(\"Filtered text corpus:\")\n",
    "print(content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stopwords found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found stopwords: \n",
      "['is', 'of', 'which', 'and', 'in', 'and', 'a', 'to', 'the', 'of', 'their', 'or', 'while', 'of', 'is', 'to', 'just', 'and', 'This', 'is', 'to', 'just', 'the', 'and', 'out', 'on', 'those', 'that', 'are', 'to', 'be', 'So', 'what', 'should', 'a', 'do', 'to', 'that']\n"
     ]
    }
   ],
   "source": [
    "# Display the identified stopwords \n",
    "stopwords_found = [words for words in tokens if words.lower() in stopwords_list]\n",
    "print(\"Found stopwords: \")\n",
    "print(stopwords_found)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Form Parts of Speech (POS) taggers & Syntactic Analysers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A videogame or computergame is an electronic-game that involves interaction with a user interface or input device\n"
     ]
    }
   ],
   "source": [
    "# Open file\n",
    "with open('Data_2.txt') as f:\n",
    "    data_2= f.read()\n",
    "print(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PAVILION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PAVILION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import re\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'videogame', 'or', 'computergame', 'is', 'an', 'electronic', 'game', 'that', 'involves', 'interaction', 'with', 'a', 'user', 'interface', 'or', 'input', 'device']\n"
     ]
    }
   ],
   "source": [
    "# Clean non-alphanumeric characters\n",
    "clean_words = re.sub(\"[^a-zA-Z]\", \" \", data_2)\n",
    "clean_words = \" \".join(clean_words.split())\n",
    "tokens = word_tokenize(clean_words)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('videogame', 'NN'), ('or', 'CC'), ('computergame', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('electronic', 'JJ'), ('game', 'NN'), ('that', 'WDT'), ('involves', 'VBZ'), ('interaction', 'NN'), ('with', 'IN'), ('a', 'DT'), ('user', 'JJ'), ('interface', 'NN'), ('or', 'CC'), ('input', 'NN'), ('device', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Conduct POS tagging using NLTK POS tagger\n",
    "tagged_tokens = pos_tag(tokens) \n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS regular expression patterns\n",
    "patterns = [\n",
    "    (r'(The|the|A|a|An|an)$', 'DT'),                # determiner\n",
    "    (r'(That|that|Which|which)$', 'WDT'),           # wh-determiner\n",
    "    (r'(And|and|But|but|Or|or)$', 'CC'),            # coordinating conjunction\n",
    "    (r'(At|at|In|in|On|on|With|with)$', 'IN'),      # coordinating conjunction\n",
    "    (r'.*ic$', 'JJ'),                               # adjectives\n",
    "    (r'.*ing$', 'VBG'),                             # gerunds\n",
    "    (r'.*ed$', 'VBD'),                              # simple past\n",
    "    (r'(.*es$)|Is|is', 'VBZ'),                      # 3rd singular present\n",
    "    (r'.*en$', 'VBN'),                              # past participle\n",
    "    (r'.*ould$', 'MD'),                             # modals\n",
    "    (r'.*\\'s$', 'NN$'),                             # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                               # plural nouns\n",
    "    (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),               # cardinal numbers\n",
    "    (r'.*', 'NN')                                   # nouns (default)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    NLTK POS           Regexp POS\n",
      "A                   DT                  DT                  \n",
      "videogame           NN                  NN                  \n",
      "or                  CC                  CC                  \n",
      "computergame        NN                  NN                  \n",
      "is                  VBZ                 VBZ                 \n",
      "an                  DT                  DT                  \n",
      "electronic          JJ                  JJ                  \n",
      "game                NN                  NN                  \n",
      "that                WDT                 WDT                 \n",
      "involves            VBZ                 VBZ                 \n",
      "interaction         NN                  NN                  \n",
      "with                IN                  IN                  \n",
      "a                   DT                  DT                  \n",
      "user                JJ                  NN                  \n",
      "interface           NN                  NN                  \n",
      "or                  CC                  CC                  \n",
      "input               NN                  NN                  \n",
      "device              NN                  NN                  \n"
     ]
    }
   ],
   "source": [
    "# Conduct POS tagging using Regular Expression tagger\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "regexp_tokens = regexp_tagger.tag(tokens)\n",
    "\n",
    "# Compare NLTK POS Tagger and Regular Expression tagger results\n",
    "print(f'{\"\":{19}} {\"NLTK POS\":{18}} {\"Regexp POS\"}')\n",
    "for nltk_result, regexp_result in zip(tagged_tokens, regexp_tokens):\n",
    "   print('{:20}{:20}{:20}'.format(nltk_result[0], nltk_result[1], regexp_result[1])) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Parse Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct parse trees\n",
    "chunker = nltk.RegexpParser(\"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>+} # Extract noun phrases\n",
    "NP: {<NP> <CC> <NP>} # Extract conjuncted noun\n",
    "P: {<IN>}            # Extract prepositions\n",
    "V: {<V.*>}           # Extract verbs\n",
    "PP: {<P> <NP>}       # Extract prepositional phrases\n",
    "VP: {<V> <NP|PP>*}   # Extract verb phrases\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (NP A/DT videogame/NN) or/CC (NP computergame/NN))\n",
      "  (VP (V is/VBZ) (NP an/DT electronic/JJ game/NN))\n",
      "  that/WDT\n",
      "  (VP\n",
      "    (V involves/VBZ)\n",
      "    (NP interaction/NN)\n",
      "    (PP\n",
      "      (P with/IN)\n",
      "      (NP\n",
      "        (NP a/DT user/JJ interface/NN)\n",
      "        or/CC\n",
      "        (NP input/NN device/NN)))))\n"
     ]
    }
   ],
   "source": [
    "# Parse the tagged tokens\n",
    "output = chunker.parse(tagged_tokens)\n",
    "\n",
    "# Print the outputs in a lexical format\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise first parse tree\n",
    "output.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "524109f4502897c0cebd03c42893e61bb28c88fa6a5398dbc2489d361e5a6984"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
